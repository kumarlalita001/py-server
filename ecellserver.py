# -*- coding: utf-8 -*-
"""ecellserver.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RB9WBTHX7NTpVf9p3n8A0hHgGYcpXJKn
"""

!pip install flask-ngrok
!pip install pyngrok==4.1.1
!ngrok authtoken 2brCNOiwwG3DDWt5dvynk9giFf1_2H4QWMYnrfwzcva5bNbRc

# import tensorflow_hub as hub
# import numpy as np
# import tensorflow as tf
# import warnings
# warnings.filterwarnings("ignore")

# model_emb = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")
# @tf.function
# def embed(input):
#   return model_emb([input])
# import torch
# import torch.nn.functional as F
# def cosine_sim(s1,s2):

#   tensor1 = embed(s1)
#   tensor2 = embed(s2)
#   tensor1 = torch.from_numpy(tensor1.numpy()[0])
#   tensor2 = torch.from_numpy(tensor2.numpy()[0])
#   tensor1 = tensor1.unsqueeze(0)
#   tensor2 = tensor2.unsqueeze(0)
#   cosine_similarity = F.cosine_similarity(tensor1, tensor2)

#   return cosine_similarity.item()

# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# tokenizer = AutoTokenizer.from_pretrained("Vamsi/T5_Paraphrase_Paws")
# model = AutoModelForSeq2SeqLM.from_pretrained("Vamsi/T5_Paraphrase_Paws")#.tocuda
# # Teacher's Master Answers
# master_answers = [
#     "The Blue Revolution is a government initiative in India focused on improving and modernizing the fisheries sector. It involves the introduction of advanced aquaculture techniques and policies to increase fish production, support fisherfolk, and ensure food security.",
#     "Hormones like estrogen and progesterone regulate the menstrual cycle. They help with things like preparing the body for a potential pregnancy, releasing an egg, and causing the uterus lining to shed if pregnancy doesn't happen.",
#     "Biodiversity is the variety of life on Earth, including different species, ecosystems, and genetic diversity within species. It's important for the stability of ecosystems and the many services they provide.",
#     "DNA is a molecule that carries genetic instructions. It's like a code with sequences of nucleotides, and it's responsible for traits passing from one generation to the next.",
#     "Cations and anions are ions with charges in chemistry. Cations are positive, anions are negative. They play a role in forming ionic compounds."
# ]

# # Revised Student Answers
# student_answers = [
#     "The Blue Revolution is like a government plan to make fish farming better. They want more fish for people to eat, so they're doing things to help fish farmers and make it all more modern.",
#     "Hormones are chemicals that help with the period cycle. They do things like getting the body ready for a possible baby and making sure everything happens when it should.",
#     "Biodiversity is all about the different living things on Earthâ€”like animals, plants, and bugs. It's important because it keeps nature diverse and working well.",
#     "DNA is like a tiny code inside cells that tells them what to do. It's important for passing on traits, like making sure kids look a bit like their parents.",
#     "Cations and anions are ions with charges in chemistry. Cations are positive, anions are negative. It's about electrons and stuff, and they help make chemical compounds."
# ]
# min_marks = 0.0
# max_marks = 10.0
# for index,ans in enumerate(master_answers):


#   text =  "paraphrase: " + ans + " </s>"

#   encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors="pt")
#   input_ids  = encoding["input_ids"]#.tocuda,
#   attention_masks = encoding["attention_mask"]#.tocuda

#   outputs = model.generate(
#       input_ids=input_ids, attention_mask=attention_masks,
#       max_length=256,
#       do_sample=True,
#       top_k=120,
#       top_p=0.95,
#       early_stopping=True,
#       num_return_sequences=5
#   )
#   max_sim = 0.0
#   for output in outputs:
#       line = tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)
#       #print(line)
#       max_sim = max(cosine_sim(line,student_answers[index]),max_sim)

#   marks = round(min(max(max_sim*max_marks,min_marks),max_marks))
#   print(f"Marks for Q{index} = {marks}")

# from flask import Flask, request, jsonify
# import tensorflow_hub as hub
# import numpy as np
# import tensorflow as tf
# import torch
# import torch.nn.functional as F
# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
# from flask_ngrok import run_with_ngrok
# app = Flask(__name__)
# run_with_ngrok(app)

# # Load Universal Sentence Encoder model
# model_emb = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

# # Define the embed function
# @tf.function
# def embed(input):
#     return model_emb([input])

# # Define the cosine similarity function
# def cosine_sim(s1, s2):
#     tensor1 = embed(s1)
#     tensor2 = embed(s2)
#     tensor1 = torch.from_numpy(tensor1.numpy()[0])
#     tensor2 = torch.from_numpy(tensor2.numpy()[0])
#     tensor1 = tensor1.unsqueeze(0)
#     tensor2 = tensor2.unsqueeze(0)
#     cosine_similarity = F.cosine_similarity(tensor1, tensor2)
#     return cosine_similarity.item()

# # Load T5 Paraphrasing model
# tokenizer = AutoTokenizer.from_pretrained("Vamsi/T5_Paraphrase_Paws")
# model = AutoModelForSeq2SeqLM.from_pretrained("Vamsi/T5_Paraphrase_Paws")

# @app.route('/')
# def home():
#   return jsonify("Hello World!")
# # Define the endpoint for computing cosine similarity
# @app.route('/compute_cosine_similarity', methods=['POST'])
# def compute_cosine_similarity():
#     data = request.get_json()

#     teachers_ans = data.get('teachers_ans', '')
#     students_ans = data.get('students_ans', '')
#     min_marks = data.get('min_marks', 0)
#     max_marks = data.get('max_marks', 100)

#     text = "paraphrase: " + teachers_ans + " </s>"
#     encoding = tokenizer.encode_plus(text, pad_to_max_length=True, return_tensors="pt")
#     input_ids = encoding["input_ids"]
#     attention_masks = encoding["attention_mask"]

#     outputs = model.generate(
#         input_ids=input_ids, attention_mask=attention_masks,
#         max_length=256,
#         do_sample=True,
#         top_k=120,
#         top_p=0.95,
#         early_stopping=True,
#         num_return_sequences=5
#     )

#     max_sim = 0.0
#     for output in outputs:
#         line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)
#         similarity = max(cosine_sim(line, students_ans),max_sim)

#     marks = min(max(max_sim*max_marks,min_marks),max_marks)
#     result = {
#         "marks": marks
#     }
#     return jsonify(result)

# if __name__ == '__main__':
#     app.run()

!pip install google-cloud-aiplatform

!pip install pypdfium2

!pip install flask-ngrok

API_KEY = api_key
OCR_KEY = ocr_key
students_ans = "https://firebasestorage.googleapis.com/v0/b/codeberg-iter.appspot.com/o/questions%2F67aa484e-1d13-4b85-92a6-2e5ddd91264a.pdf?alt=media&token=1d92918f-e428-49ad-b352-92e80eadafbf"
students_ans = ocr_on_pdf(students_ans)[0]
predict_large_language_model_sample("decent-lambda-331815", "text-bison@001", 0.6, 256, 0.8, 40, f'Tell me in brief how i can improve the answers i have written in my exam paper. The answers are:{students_ans}', "us-central1")

!pip install prophet --no-binary :all:

import pypdfium2 as pdfium
import os
import requests
import shutil
import re
import requests
import base64
import json
def convert_to_image(filename):
  pdf = pdfium.PdfDocument(filename)
  n_pages = len(pdf)
  for i in range(n_pages):
    page = pdf.get_page(i)
    pil_image = page.render(scale = 300/72).to_pil()
    image_name =f"{i}.jpg"
    # Get the current working directory
    current_directory = os.getcwd()

    # Specify the target directory (in this case, 'tmp' directory)
    target_directory = os.path.join(current_directory, "tmp")

    # Ensure the target directory exists, create it if not
    os.makedirs(target_directory, exist_ok=True)

    # Save the image to the 'tmp' directory with the specified name
    pil_image.save(os.path.join(target_directory, image_name))

def ocr_on_pdf(link):
  ocr_res = []
  # URL of the PDF file
  pdf_url = link

  # Create a temporary directory
  tmp_dir = "tmp"
  os.makedirs(tmp_dir, exist_ok=True)

  try:
      # Download the PDF to the temporary directory using requests
      pdf_filename = os.path.join(tmp_dir, "downloaded_pdf.pdf")
      with open(pdf_filename, 'wb') as pdf_file:
          response = requests.get(pdf_url)
          pdf_file.write(response.content)

      # Convert the PDF to images
      images = convert_to_image(pdf_filename)

  finally:
      # Delete the PDF file
      os.remove(pdf_filename)
      for filename in sorted(os.listdir(tmp_dir)):
        f = os.path.join(tmp_dir, filename)
        s = re.sub(r'\n', '', file_ocr(f))
        ocr_res.append(s)
      # Delete the temporary directory
      shutil.rmtree(tmp_dir)
      return ocr_res

      ###constraint len masdter == len student


def file_ocr(image_path):
    # Encode the image as base64
    with open(image_path, 'rb') as image_file:
        encoded_image = base64.b64encode(image_file.read()).decode('utf-8')

    # Google Cloud Vision API endpoint
    endpoint = 'https://vision.googleapis.com/v1/images:annotate?key=' + OCR_KEY

    # Request payload
    request_data = {
        'requests': [
            {
                'image': {'content': encoded_image},
                'features': [{'type': 'TEXT_DETECTION'}],
            }
        ]
    }

    # Make a POST request to the API
    response = requests.post(endpoint, json=request_data)

    # Parse the response
    if response.status_code == 200:
        result = response.json()
        if 'textAnnotations' in result['responses'][0]:
            for annotation in result['responses'][0]['textAnnotations']:
                return(annotation['description'])
        else:
            print("No text detected.")
    else:
        print(f"Error: {response.status_code}, {response.text}")

from google.colab import auth as google_auth
google_auth.authenticate_user()

import vertexai
from vertexai.preview.language_models import TextGenerationModel

def predict_large_language_model_sample(
    project_id: str,
    model_name: str,
    temperature: float,
    max_decode_steps: int,
    top_p: float,
    top_k: int,
    content: str,
    location: str = "us-central1",
    tuned_model_name: str = "",
    ) :
    """Predict using a Large Language Model."""
    vertexai.init(project=project_id, location=location)
    model = TextGenerationModel.from_pretrained(model_name)
    if tuned_model_name:
      model = model.get_tuned_model(tuned_model_name)
    response = model.predict(
        content,
        temperature=temperature,
        max_output_tokens=max_decode_steps,
        top_k=top_k,
        top_p=top_p,)
    return response.text

import pandas as pd
from prophet import Prophet
import matplotlib.pyplot as plt
from io import BytesIO
import base64

def generate_dummy():
  # Generate random time series data
  np.random.seed(42)  # Set seed for reproducibility

  start_date = pd.to_datetime('2022-01-01')
  end_date = pd.to_datetime('2022-05-01')
  date_range = pd.date_range(start=start_date, end=end_date, freq='MS')

  random_data = {
      'ds': date_range,
      'y': np.random.randint(50, 80, size=len(date_range))
  }

  df = pd.DataFrame(random_data)

  # Create and fit the Prophet model
  model = Prophet()
  model.fit(df)

  # Generate future dates for prediction
  future = model.make_future_dataframe(periods=12, freq='M')  # Adjust the number of periods as needed

  # Make predictions
  forecast = model.predict(future)

  # Plot the forecast
  fig = model.plot(forecast)
  plt.xlabel('Date')
  plt.ylabel('Marks')
  plt.title('Prophet Time Series Forecasting')

  # Save the plot to a BytesIO object
  image_stream = BytesIO()
  plt.savefig(image_stream, format='png')
  plt.close()

  # Encode the BytesIO object to base64
  encoded_image = base64.b64encode(image_stream.getvalue()).decode('utf-8')

  # Add the base64-encoded image to the forecast DataFrame
  forecast['encoded_image'] = encoded_image

  # Convert the forecast DataFrame to JSON
  forecast_json = forecast.to_json(orient='records')

  # Now, you can use `forecast_json` which includes the base64-encoded image
  return forecast_json

from flask import Flask, request, jsonify
import os
import requests
from transformers import pipeline
import tensorflow_hub as hub
import numpy as np
import tensorflow as tf
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from flask_ngrok import run_with_ngrok
app = Flask(__name__)
run_with_ngrok(app)


# Load Universal Sentence Encoder model
model_emb = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

# Define the embed function
@tf.function
def embed(input):
  #this is added to remove the issue of multiple pages in text
  s = ""
  for page in input:
    s += page
  return model_emb([s])

# Define the cosine similarity function
def cosine_sim(s1, s2):
    tensor1 = embed(s1)
    tensor2 = embed(s2)
    tensor1 = torch.from_numpy(tensor1.numpy()[0])
    tensor2 = torch.from_numpy(tensor2.numpy()[0])
    tensor1 = tensor1.unsqueeze(0)
    tensor2 = tensor2.unsqueeze(0)
    cosine_similarity = F.cosine_similarity(tensor1, tensor2)
    return cosine_similarity.item()

# Load T5 Paraphrasing model
tokenizer = AutoTokenizer.from_pretrained("Vamsi/T5_Paraphrase_Paws")
model = AutoModelForSeq2SeqLM.from_pretrained("Vamsi/T5_Paraphrase_Paws")

@app.route('/mark_answer_sub', methods=['POST'])
def compute_cosine_similarity():
    data = request.get_json()

    teachers_ans = data.get('teachers_ans', '')
    students_ans = data.get('students_ans', '')
    min_marks = data.get('min_marks', 0)
    max_marks = data.get('max_marks', 100)
    teachers_ans = ocr_on_pdf(teachers_ans)[0]
    students_ans = ocr_on_pdf(students_ans)[0]

    text = "paraphrase: " + teachers_ans + " </s>"
    encoding = tokenizer.encode_plus(text, pad_to_max_length=True, return_tensors="pt")
    input_ids = encoding["input_ids"]
    attention_masks = encoding["attention_mask"]

    outputs = model.generate(
        input_ids=input_ids, attention_mask=attention_masks,
        max_length=256,
        do_sample=True,
        top_k=120,
        top_p=0.95,
        early_stopping=True,
        num_return_sequences=5
    )

    max_sim = 0.0
    for output in outputs:
        line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)
        max_sim = max(cosine_sim(line, students_ans), max_sim)

    marks = min(max(max_sim * max_marks, min_marks), max_marks)
    result = {
        "marks": marks
    }
    return jsonify(result)

@app.route('/mark_answer_obj', methods=['POST'])
def compute_cosine_similarity_obj():
    data = request.get_json()

    teachers_ans = data.get('teachers_ans', '')
    students_ans = data.get('students_ans', '')
    marks = data.get('marks', 0)
    teachers_ans = ocr_on_pdf(teachers_ans)[0]
    students_ans = ocr_on_pdf(students_ans)[0]

    if cosine_sim(teachers_ans,students_ans) >= 0.9:
      marks_obt = marks
    else:
      marks_obt = 0.0
    result = {
        "marks": marks_obt
    }
    return jsonify(result)

@app.route('/check_plag', methods=['POST'])
def check_plag():
    # Get input text from the request
    links = request.json.get('links', [])
    entire_class_text = []
    result = []
    pipe = pipeline("text-classification", model="roberta-base-openai-detector")
    # API_URL and headers
    API_URL = "https://api-inference.huggingface.co/models/jpwahle/longformer-base-plagiarism-detection"
    headers = {"Authorization": f"Bearer {API_KEY}"}
    for link in links:
      score = 0.0
      text = ocr_on_pdf(link)
      for page in text:
        if len(page) > 512:
          page = page[:512]
          classification_result = pipe(page)
          classification_label = classification_result[0]["label"]
          classification_score = classification_result[0]["score"]
          if classification_label == "Real":
            score += classification_score
          else:
            score -= classification_score
        result_class = score/len(text)
    # Query the plagiarism detection model
      payload = {"inputs": text}
      response = requests.post(API_URL, headers=headers, json=payload)
      plagiarism_result = response.json()
      score_pg = plagiarism_result[0][0]['score']
      res = {
          "AI_Gen": result_class,
          "Plagiarised": score_pg,
          "Copied":"False"
      }
      result.append(res)
      entire_class_text.append(text)
    n = len(entire_class_text)
    for i in range(n):
        for j in range(i + 1, n):
            if cosine_sim(entire_class_text[i],entire_class_text[j]) >= 0.8:
                result[i]["Copied"] = "True"
                result[j]["Copied"] = "True"
    return jsonify(result)
@app.route('/ocr',methods = ['POST'])
def ocr_only():
  link = request.json.get('link', ' ')
  res = ocr_on_pdf(link)
  return jsonify({"result" : res})

@app.route('/feedback',methods = ['POST'])
def give_feedback():
  data = request.get_json()
  students_ans = data.get('students_ans', '')
  students_ans = ocr_on_pdf(students_ans)
  print(students_ans)
  res = predict_large_language_model_sample("decent-lambda-331815", "text-bison@001", 0.6, 256, 0.8, 40, f'Tell me in brief how i can improve the answers i have written in my exam paper. The answers are:{students_ans}', "us-central1")
  return jsonify({"result" : res})

@app.route('/dummy_image',methods = ['POST'])
def gen():
  return jsonify({"result" : generate_dummy()})

# @app.route('/ocr', methods=['POST'])
# def ocr_endpoint():
#     data = request.get_json()

#     # Get the image URL from the request
#     image_url = data.get('image_url', '')

#     # Perform OCR on the image
#     ocr_result = perform_ocr_with_api_key(image_url)

#     result = {
#         "ocr_result": ocr_result
#     }

#     return jsonify(result)

if __name__ == '__main__':
    app.run()

